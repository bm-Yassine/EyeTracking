Metadata-Version: 2.4
Name: eyetracker
Version: 0.1.0
Summary: Webcam eye tracker: calibration & gaze pipeline
Author: Yassine Bendimerad
License: CTU
Requires-Python: >=3.10
Description-Content-Type: text/markdown
Requires-Dist: opencv-contrib-python>=4.8
Requires-Dist: numpy>=1.24
Requires-Dist: pyyaml>=6.0
Requires-Dist: pydantic>=2.6
Requires-Dist: pyarrow>=16.0
Provides-Extra: mediapipe
Requires-Dist: mediapipe>=0.10.9; extra == "mediapipe"
Provides-Extra: spiga
Requires-Dist: torch>=2.2; extra == "spiga"
Provides-Extra: pandas
Requires-Dist: pandas>=2.2; extra == "pandas"
Provides-Extra: dev
Requires-Dist: pytest>=7; extra == "dev"
Requires-Dist: ruff>=0.4; extra == "dev"
Requires-Dist: mypy>=1.7; extra == "dev"
Requires-Dist: types-PyYAML; extra == "dev"
Requires-Dist: types-opencv-python; extra == "dev"

Eye Tracker
A webcam-based eye tracking and calibration toolkit with support for MediaPipe FaceMesh/Iris, SPIGA (adapter-ready), and ChArUco/Checkerboard camera calibration.
It provides full-session logging (frames, events, video), head pose estimation, iris/pupil features, and real-time quality checks.
ğŸ“¦ Installation
pip install -e .
ğŸ¯ Calibration
ChArUco (default)
eye-cam-calibrate --camera-id 0 --width 1280 --height 720
Checkerboard
eye-cam-calibrate --pattern checkerboard --cb-cols 9 --cb-rows 6 --cb-square-mm 25
ğŸš€ Run the Calibration UI
python -m eyetracker.app.calibrate --backend none --hud
python -m eyetracker.app.calibrate --backend mediapipe --hud --record
CLI Flags
  Flag          Description	                                   Default
--config <path>	Path to YAML config	                            configs/default.yaml
--seed <int>	  Seed for grid order	                            17
--hud	Show      on-screen HUD (FPS, backend, point idx, flags)	Off
--backend       {mediapipe... ,none}	Landmark backend	        mediapipe
--record	      Save webcam video to session folder	            Off
ğŸ—‚ Project Structure
eye-tracker/
â”œâ”€ pyproject.toml        # Dependencies + entry points
â”œâ”€ README.md
â”œâ”€ LICENSE
â”œâ”€ configs/
â”‚   â””â”€ default.yaml      # Camera ID, screen size, backend toggle
â”œâ”€ data/
â”‚   â”œâ”€ sessions/         # Per-session logs (Parquet/CSV + MP4)
â”‚   â””â”€ camera/           # Camera intrinsics (json/yaml)
â”œâ”€ eyetracker/           # Python package
â”‚   â”œâ”€ app/              # CLI entry points
â”‚   â”œâ”€ ui/               # Calibration grid + HUD rendering
â”‚   â”œâ”€ video/            # Capture + writer with overlays
â”‚   â”œâ”€ vision/           # Landmark backends + headpose
â”‚   â”œâ”€ features/         # Iris, pupil, gaze geometry
â”‚   â”œâ”€ io/               # Logging + schemas
â”‚   â”œâ”€ quality/          # Real-time quality gates
â”‚   â””â”€ utils/            # Timing, screen helpers
â”œâ”€ scripts/              # Demos + exports
â””â”€ tests/                # Unit tests
âœ… Quick Tests
1. UI Only (no webcam)
python -m eyetracker.app.calibrate --backend none --hud
Full-screen black canvas with a white (current) and red (next) cross.
Click through 25 points â†’ session saved in data/sessions/.
2. Webcam + MediaPipe
python -m eyetracker.app.calibrate --backend mediapipe --hud --record
HUD shows FPS, face presence, blink detection.
Session folder includes frames.parquet, events.parquet, video.mp4.
ğŸ§ª Features & Checks
Camera Calibration: ChArUco or checkerboard intrinsics.
Head Pose: Robust PnP â†’ yaw, pitch, roll, distance.
Eye Features: Iris circle fit, pupil angles, blinks.
Session Logging: Frames/events saved as Parquet, optional MP4.
Quality Gates: Face present, blink detection, motion sanity.



A0: deps + a camera-intrinsics module with a CLI to calibrate your webcam (checkerboard or ChArUco), save K & dist to data/camera/, and utilities to load/undistort later.
A1: grid order + current/next rendering + click logging + HUD.
A2: threaded capture with get_latest() and MP4 writer with overlay hook.
A3: MediaPipe FaceMesh+Iris backend (standardized outputs); SPIGA adapter stub for later SOTA integration.
A4 â€“ vision/headpose.py: robust PnP from a fixed 6-point model â†’ yaw/pitch/roll (deg) + distance_mm.
A5 â€“ features/geometry.py: iris circle fit (already used), eye reference (corner midpoint), and the exact pinhole mapping(with image-y inverted so â€œupâ€ is +pitch).
A6 â€“ features/feature_builder.py + io/schemas.py + io/logger.py: per-frame struct with timestamps, head pose, left/right pupil angles, iris radii, normalized eye-corner geometry, quality flags, and target at click frames; streamed to Parquet (frames.parquet, events.parquet).
A7 â€“ quality/gates.py: lightweight real-time gatesâ€”face present and a blink surrogate based on iris-radius-to-eye-width ratio.
A8 â€“ app/calibrate.py writes a full session folder:
data/sessions/calib_YYYYmmdd_HHMMSS/
  â”œâ”€ calib_points.json      # order, screen, seed
  â”œâ”€ frames.parquet         # every frame
  â”œâ”€ events.parquet         # one row per click
  â”œâ”€ video.mp4              # if --record
  â””â”€ config_snapshot.yaml
